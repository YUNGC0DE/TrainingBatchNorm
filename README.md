# Training BatchNorm and only BatchNorm

Эта реализация статьи https://arxiv.org/pdf/2003.00152.pdf показывает возможность обучения CIFAR10 при помощи <br>
рандомно ициализированных весов и только при помощи них.

В оригинальной статье предложено сравнение точности полученной модели на различных архитектурах:
 <ul>
 <li>ResNet(N), где N принадлежит (14, 32, 56, 110, 218, 434, 866) </li>
 <li>ResNet14-W, где W принадлежит (1, 2, ,4 ,8, 16, 32)</li>
 <li>VGG</li>
 </ul>
 <br>
И при различных обучаемых параметрах:
<ul>
    <li>Обучение только BatchNorm </li>
    <li>Обучение всех параметров модели</li>
    <li>Обучение рандомно взятых параметров сверточных слоев (кол-во пропорционально BN параметрам)</li>
</ul>

В данной реализации рассмотрены только ResNet<32, 56, 110> для сравнения BN-only с пропорциональным колличестом<br>
рандомных параметров сети

Увеличение глубины способствует приросту качества из-за увелечения общего числа обучаемых параметров. 
Но даже при увеличении глубины BN параметры не занимают больше 0.5% сети и при этом дают относительно хорошее качество
<table>
<tr>
<td>
</td>
<td>
ResNet32
</td>
<td>
ResNet56
</td>
<td>
ResNet101
</td>
</tr>
<tr>
<td>
<b>Total Params</b>
</td>
<td>
467194
</td>
<td>
856058
</td>
<td>
1731002
</td>
</tr>
<tr>
<td>
<b>BN Trainable</b>
</td>
<td>
2272
</td>
<td>
4064
</td>
<td>
8096
</td>
</tr>
<tr>
<td>
<b>%</b>
</td>
<td>
0.49%
</td>
<td>
0.48%
</td>
<td>
0.47%
</td>
</tr>
</table>
<br>
<b>Сравнение качества ResNet с различной глубиной:</b><br>

![image](images/ResNets.png)
<br>
<br>
Как можно заметить, обучение только BN даёт более хорошее качество по сравнению с рандомными параметрами сети (2 F.P.C)
<br>

Такое поведение,вероятно, обусловленно тем, что обучаются слои целиком, нежели отдельные параметры слоев


